{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kullback–Leibler divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In mathematical statistics, the Kullback–Leibler divergence (also called relative entropy) is a measure of how one probability distribution is different from a second, reference probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KL Divergence has its origins in information theory. The primary goal of information theory is to quantify how much information is in data. The most important metric in information theory is called Entropy, typically denoted as $H$. The definition of Entropy for a probability distribution is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$H = -\\sum_{i=1}^{N} p(x_i) \\cdot \\text{log }p(x_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kullback-Leibler Divergence is just a slight modification of our formula for entropy. Rather than just having our probability distribution \\(p\\) we add in our approximating distribution \\(q\\). Then we look at the difference of the log values for each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$D_{KL}(p||q) = \\sum_{i=1}^{N} p(x_i)\\cdot (\\text{log }p(x_i) - \\text{log }q(x_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more common way to see KL divergence written is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$D_{KL}(p||q) = \\sum_{i=1}^{N} p(x_i)\\cdot log\\frac{p(x_i)}{q(x_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With KL divergence we can calculate exactly how much information is lost when we approximate one distribution with another. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL divergence between two univariate Gaussians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "KL(p, q) &= - \\int p(x) \\log q(x) dx + \\int p(x) \\log p(x) dx\\\\\\\\\n",
    "&=\\frac{1}{2} \\log (2 \\pi \\sigma_2^2) + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2 \\sigma_2^2} - \\frac{1}{2} (1 + \\log 2 \\pi \\sigma_1^2)\\\\\\\\\n",
    "&= \\log \\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2 \\sigma_2^2} - \\frac{1}{2}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stats.stackexchange.com/a/7449"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL divergence example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can go ahead and calculate the KL divergence for our two approximating distributions. For the uniform distribution we find:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$D_{kl}(\\text{Observed } || \\text{ Uniform}) = 0.338$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for our Binomial approximation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$D_{kl}(\\text{Observed } || \\text{ Binomial}) = 0.477$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the * information lost * by using the Binomial approximation is greater than using the uniform approximation. If we have to choose one to represent our observations, we're better off sticking with the Uniform approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross entropy and KL divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/kl_decomp.png' width=400px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross entropy is a combination of the entropy of the 'true' distribution $P$ and the KL divergence between $P$ and $Q$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$H(p, q) = H(p) + D_{KL}(p \\parallel q)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table >\n",
    "\n",
    "<tbody><tr>\n",
    "<th>x</th>\n",
    "<th>0</th>\n",
    "<th>1</th>\n",
    "<th>2\n",
    "</th></tr>\n",
    "<tr>\n",
    "<td>Distribution <i>P</i>(x)</td>\n",
    "<td>0.36</td>\n",
    "<td>0.48</td>\n",
    "<td>0.16\n",
    "</td></tr>\n",
    "<tr>\n",
    "<td>Distribution <i>Q</i>(x)</td>\n",
    "<td>0.333</td>\n",
    "<td>0.333</td>\n",
    "<td>0.333\n",
    "</td></tr></tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/600px-Kullback–Leibler_distributions_example_1.svg.png' width='400px' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KL divergences $D_{KL}(p||q)$ and $D_{KL}(q||p)$ are calculated as follows. This example uses the natural log with base e, designated $ln$ to get results in nats (see units of information)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/f7e334c49da344f0758d66e3147ee46447a715bf.svg' width='500px'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/2cca3f4865cb115e739c9df38eaedee69f432600.svg' width='500px'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "* Wikipedia https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\n",
    "* Kullback-Leibler Divergence Explained https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained\n",
    "* An introduction to entropy, cross entropy and KL divergence in machine learning https://adventuresinmachinelearning.com/cross-entropy-kl-divergence/\n",
    "* 정보이론2편,: KL-Divergence, https://brunch.co.kr/@chris-song/69\n",
    "* KL divergence between two univariate Gaussians, URL (version: 2016-07-21): https://stats.stackexchange.com/q/7449"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py35_keras",
   "language": "python",
   "name": "py35_keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
