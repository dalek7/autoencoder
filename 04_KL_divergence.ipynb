{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In mathematical statistics, the Kullback–Leibler divergence (also called relative entropy) is a measure of how one probability distribution is different from a second, reference probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KL Divergence has its origins in information theory. The primary goal of information theory is to quantify how much information is in data. The most important metric in information theory is called Entropy, typically denoted as $H$. The definition of Entropy for a probability distribution is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$H = -\\sum_{i=1}^{N} p(x_i) \\cdot \\text{log }p(x_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kullback-Leibler Divergence is just a slight modification of our formula for entropy. Rather than just having our probability distribution \\(p\\) we add in our approximating distribution \\(q\\). Then we look at the difference of the log values for each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$D_{KL}(p||q) = \\sum_{i=1}^{N} p(x_i)\\cdot (\\text{log }p(x_i) - \\text{log }q(x_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more common way to see KL divergence written is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$D_{KL}(p||q) = \\sum_{i=1}^{N} p(x_i)\\cdot log\\frac{p(x_i)}{q(x_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With KL divergence we can calculate exactly how much information is lost when we approximate one distribution with another. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can go ahead and calculate the KL divergence for our two approximating distributions. For the uniform distribution we find:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$D_{kl}(\\text{Observed } || \\text{ Uniform}) = 0.338$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for our Binomial approximation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$D_{kl}(\\text{Observed } || \\text{ Binomial}) = 0.477$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the * information lost * by using the Binomial approximation is greater than using the uniform approximation. If we have to choose one to represent our observations, we're better off sticking with the Uniform approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table >\n",
    "\n",
    "<tbody><tr>\n",
    "<th>x</th>\n",
    "<th>0</th>\n",
    "<th>1</th>\n",
    "<th>2\n",
    "</th></tr>\n",
    "<tr>\n",
    "<td>Distribution <i>P</i>(x)</td>\n",
    "<td>0.36</td>\n",
    "<td>0.48</td>\n",
    "<td>0.16\n",
    "</td></tr>\n",
    "<tr>\n",
    "<td>Distribution <i>Q</i>(x)</td>\n",
    "<td>0.333</td>\n",
    "<td>0.333</td>\n",
    "<td>0.333\n",
    "</td></tr></tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/600px-Kullback–Leibler_distributions_example_1.svg.png' width='400px' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KL divergences $D_{KL}(p||q)$ and $D_{KL}(q||p)$ are calculated as follows. This example uses the natural log with base e, designated $ln$ to get results in nats (see units of information)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/f7e334c49da344f0758d66e3147ee46447a715bf.svg' width='500px'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/2cca3f4865cb115e739c9df38eaedee69f432600.svg' width='500px'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "* Wikipedia https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\n",
    "* Kullback-Leibler Divergence Explained https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py35_keras",
   "language": "python",
   "name": "py35_keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
